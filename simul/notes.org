
				Simulation Notes

* Navigation
  C-c C-l -- edit a link (useful for copying)
  C-c C-o -- follow a link

* TODO [2/7]
** TODO [0/1] Object Creation
Develop a method for creating functions that obey the growth property of the
paper

  c(threads) = base + incremental * (threads - 1)

*** TODO Dynamic Object Creation
It may be easiest to develop a dynamic function that takes parameters

  object(int base, int incremental, int FACTOR)

Where base, incremental correspond to the growth property WCET function.
FACTOR is the "unit of cache consumption", possibly the number of cache
blocks.

If base = 10, and incremental = 4, and FACTOR = 3, where the cache block size
is 2 words. Then

  object(10, 4, 3) will access 42 cache blocks
** TODO print
Implement a print method for use only on core 0 to print messages to the
UART. It does not need to be fancy, just print a NULL terminated string

  print(char *s);

    or

  printn(int len, char *s);

** TODO Automate Task Generation
Write a python script that takes as input the core schedules and produces the C
source files to represent the execution of the fork-join task.

** DONE Port the MRTC benchmarks used by BUNDLE
Each of the benchmarks needs to be ported to a single object.

** TODO [1/3] Measure the growth factor of the benchmarks
After porting the MRTC benchmarks, create a test case for each which will
determine the base and incremental costs. This is an emperical result, but it
must be *agnostic* of the cache configuration.

*** DONE Code conversion
Compatible source declaration in the lock-step protocol

*** TODO Averaging
- [ ] Automatically running and collecting the per benchmark results
- [ ] Average the results over 100 or more runs in an automated manner

*** TODO Floating Point
Floating point operations are failing, not sure why. Alleyn's current task.

** DONE Instruction Cache Mapping
The cache layout and mapping is not explicit from the virt documentation. Is it
simple address separation? Yes, see [[cache_mapping]]

** TODO [3/3] Quantify the results
See bin/cycles.py
*** DONE Extract
Extract the number of instruction hits and misses per core.
*** DONE Evaluate
Using estimated block reload times (BRT) and cycles per instruction (1) evaluate
the number of cycles per core (excluding core 0).
*** DONE Tabulate
Tabulate the results per core, with the number of total cycles and the achieved
cache benefit in terms of cycles saved.




* Current Reference
[[https://twilco.github.io/riscv-from-scratch/2019/04/27/riscv-from-scratch-2.html]]
** Next Step
[[https://mth.st/blog/riscv-qemu/]]

* Cross Compiler on Gentoo

** Install crossdev first
[[https://wiki.gentoo.org/wiki/Crossdev]]

1. Install crossdev.
   ► emerge sys-devel/crossdev -av

2. Create an ebuild repository to prevent crossdev from polluting the native
   repository.
   # mkdir -p /var/db/repos/crossdev/{profiles,metadata}
   # echo 'crossdev' > /var/db/repos/crossdev/profiles/repo_name
   # echo 'masters = gentoo' > /var/db/repos/crossdev/metadata/layout.conf
   # chown -R portage:portage /var/db/repos/crossdev

3. Inform Portage of the repository.
   Create a /etc/portage/repos.conf/crossdev.conf with the following contents:
   [crossdev]
   location = /var/db/repos/crossdev
   priority = 10
   masters = gentoo
   auto-sync = no

** Installing the RISC-V toolchain(s)
When invoking crossdev for RISCV *always* include the -S option with the
command. The tuples to use are riscv32-elf and riscv64-elf. The elf portion of
the tuple indicates that there is no operating system, that the compiler is
intended for bare-metal. These will use newlib as their libc.

1. Install the 32bit cross compiler
   ► crossdev -S riscv32-elf
2. Install the 64bit cross compiler
   ► crossdev -S riscv64-elf

*** System Tuples
Recently, thankfully, someone has authored a page describing the tuples: [[https://wiki.gentoo.org/wiki/Embedded_Handbook/Tuples][Tuples]]
(C-c C-l to easily view the link)

** Removing a toolchain
1. Determine which toolchains are installed.
   ► ls -d /var/db/repos/crossdev/cross-*
   /var/db/repos/crossdev/cross-riscv32-elf
   /var/db/repos/crossdev/cross-riscv64-elf
2. --clean the --target to be removed
   ► crossdev --clean --target riscv64-elf

* QEMU
There are two types of emulation QEMU provides, userspace which virtualizes only
a process, and softmmu which emulates a complete system. softmmu is what we
want, installing both is not problematic.

** Building with emerge *FAILS*

1. Add the targets to /etc/portage/make.conf
   ► grep QEMU /etc/portage/make.conf
   QEMU_SOFTMMU_TARGETS="riscv32 riscv64"
   QEMU_USER_TARGETS="riscv32 riscv64"
2. Build QEMU
   Make sure the plugins USE flag is set
   ► emerge app-emulation/qemu -av

This does not result in a version of QEMU with the plugins built.

** Build from source *FAILS*
https://github.com/qemu/qemu/archive/refs/tags/v7.2.2.tar.gz

1. wget https://github.com/qemu/qemu/archive/refs/tags/v7.2.2.tar.gz
2. tar -xzvf v7.2.2.tar.gz
3. cd qemu-7.2.2
4. mkdir build
5. cd build
6. ./configure
   FAILS

** Build from GIT
1. Clone the repository
   git clone https://gitlab.com/qemu-project/qemu.git qemu.git
2. Switch to the tag of the latest stable release
   Current it is 8.0.0 revision c1eb2ddf
   > cd qemu.git
   > git checkout c1eb2ddf
3. Make an out-of-tree directory
   > mkdir build
4. Configure
   > cd build
   > ../configure --prefix=${HOME}/bin/qemu-8.0.0
   --target-list="riscv32-softmmu riscv64-softmmu riscv32-linux-user riscv64-linux-user"
   --enable-plugins --enable-kvm
   --enable-multiprocess --enable-qcow1 --enable-qed --enable-sdl --enable-tcg
   --enable-tools --disable-sdl
5. Build
   > make
6. Build the plugins (unsure why they aren't by default)
   > cd contrib/plugins
   > make

** Device Tree Blob (dtb)
These files describe the device QEMU is emulating.

Dumping a blob:
  ► qemu-system-riscv32 -machine virt -machine dumpdtb=risc32-virt.dtb

The dtb file is not human readable. Use dtc to convert the dtb into something
readable.
  ► dtc -I dtb -O dts -o risc32-virt.dts risc32-virt.dtb

Looking into the dts file, we can find out information about the QEMU virt
machine, including the starting address:

<<< SNIP >>>
        memory@80000000 {
                device_type = "memory";
                reg = <0x00 0x80000000 0x00 0x8000000>;
        };
<<< SNIP >>>

The reg descri ptor says that memory starts at 0x00 0x80000000, or just
0x80000000. It is 0x00 0x8000000 bytes long, or 0x00 0x8000000. Simply put

  Memory Start: 0x80000000
  Memory   End: 0x88000000

** Linker Script
Dumping the default linker script:
  ► riscv32-elf-ld --verbose > riscv32-virt.ld

** Starting QEMU
The complete command is:
  ► qemu-system-riscv32 -nographic -machine virt -bios none -m 128M \
      -S -s -smp 2 -kernel riscv32-add-v2

The options are
  -nographic       -- disable the graphical display
  -machine virt    -- choose the machine to emulate (virt)
  -bios none       -- disable the opensbi bios
  -m 128M          -- 128M of ram (that's all that's in the dtb)
  -S               -- freeze CPU on start
  -s               -- "shortcut" for "-gdb tcp:1234"
  -kernel image    -- the thing to load

Note, the "-gdb tcp:1234" option does not work, period. Only the "-s" option
works.

** Starting QEMU with the cache plugin
After the other ${options}
  ► qemu-system-riscv32 ${options}
     --plugin contrib/plugins/libcache.so,dcachesize=8192,dassoc=4,dblksize=64,icachesize=8192,iassoc=4,iblksize=64 -d plugin -D test.log

* OpenOCD
  > emerge openocd

Make sure the user is part of the plugdev group
  # usermod -a -G plugdev ct

* Separating Core Execution
Multiple cores executing out of the same in memory image is not a particularly
difficult problem, except for global variables, and the stack. It is probable
that the correct solution is to run processes and memory virtualization.

The naive approach is to divide the remaining memory space into per core stacks,
global variables will break and are therefore *not* permitted in functions. The
linker puts the BSS at the end of assigned memory, and "the" stack starts at the
bottom of all memory. In this approach, the remaining memory is divided into one
region per core as illustrated with the following four core example

| Earlier            |            |
| sections           |            |
|--------------------+------------|
| BSS                |            |
|--------------------+------------|
| __global_pointer_1 | for core 1 | 1024 bytes
| hart_id            |            |
| __global_pointer_2 | for core 2 | 1024 bytes
| hart_id            |            |
| __global_pointer_3 | for core 3 | 1024 bytes
| hart_id            |            |
| __global_pointer_4 | for core 4 | 1024 bytes
|--------------------+------------|
| Start of Stacks    | ADDRESS S  |
|--------------------+------------|
| bottom of stack 1  |            |
| ...                | core 1     | (T - S) / 4 bytes
| top of stack 1     |            |
|--------------------+------------|
| bottom of stack 2  |            |
| ...                | core 2     | (T - S) / 4 bytes
| top of stack 2     |            |
|--------------------+------------|
| bottom of stack 3  |            |
| ...                | core 3     | (T - S) / 4 bytes
| top of stack 3     |            |
|--------------------+------------|
| bottom of stack 4  |            |
| ...                | core 4     | (T - S) / 4 bytes
| top of stack 4     |            |
|--------------------+------------|
| End of Memory      | ADDRESS T  |

The stack space for each core is an equal portion of the remaining space after
the global pointers. Once a hart is initialized it's gp register will have the
value of its global_pointer for "quick" access to its hart_id and any other
information we determine is necessary.

This is the ideal, however we are not there yet. Getting a working start, using
the QEMU virt target that can run up to 8 cores. The code works for two cores,
successfully.

** risc32-two-threads
This should really be named two cores, but here we are. The important pieces:

1. On each core gp points to the start of 1024 bytes of reserved memory for the
   core. These come from a block with the address of __global_pointer_start
   defined in the linker file.

   If __global_pointer_start = 0x5000
   Then
      core 1 gp = 0x5400
      core 2 gp = 0x5800

   This is calculated in crt0.s

2. Each core has a dedicated stack with 4 megabytes of memory (0x400000). The
   last address in memory is 0x88000000 the tops of the stacks are then

   core 1 top = 0x88000000
   core 2 top = 0x87C00000

   This is calculated in crt0.s

*** Verifying and debugging
Verifying the proof of concept

1. Build it
   > make

   # This might fail, I'm not sure why but the linker complains every now and
   # again. Just run it again, seriously, I have no better solution.

2. Start QEMU compiled from git, using the new kernel
   Replace ${QEMU_GIT_CHECKOUT} with the absolute path to the git checkout that
   was used to build.

   qemu-system-riscv32 -nographic -machine virt -bios none -m 128M -S -s -smp 4 \
   --plugin ${QEMU_GIT_CHECKOUT}/build/contrib/plugins/libcache.so,icachesize=8192,iassoc=4,iblksize=64 \
   -d plugin -D cache.log -kernel riscv32-two-threads

3. Connect with the debugger.
   # in a separate terminal
   > riscv32-elf-gdb --tui riscv32-two-threads
   (gdb) layout next
   (gdb) layout next # do it twice
   (gdb) target extended-remote :1234
   (gdb) add-inferior
   (gdb) add-inferior
   (gdb) add-inferior # once per core
   (gdb) b main
   (gdb) c
   # step and continue to your enjoyment

   At some point Thread 3 (and Thread 4) will call ebreak, this is a halting
   instruction to avoid any additional actions. Select a thread that can run to
   continue.

   (gdb) info threads
   Thread 1.4 received signal SIGINT, Interrupt.
   0x00000000 in ?? ()
   (gdb) info threads
     Id   Target Id                    Frame
     1.1  Thread 1.1 (CPU#0 [running]) path_core_two (x=42) at two-threads.c:53
     1.2  Thread 1.2 (CPU#1 [running]) path_core_one (x=27) at two-threads.c:44
     1.3  Thread 1.3 (CPU#2 [running]) 0x00000000 in ?? ()
   * 1.4  Thread 1.4 (CPU#3 [running]) 0x00000000 in ?? ()

   (gdb) thread 1
   # continue the exercise

4. Verify the cache results
   The cache results can only be verified after halting the emulator. In the
   terminal where QEMU was started, Ctrl-a x. The following is printed

   QEMU: Terminated

   Now look at the cache.log file.
   ► head cache.log
   core #, data accesses, data misses, dmiss rate, insn accesses, insn misses, imiss rate
   0       17164942       4               0.0000%  34329901       4               0.0000%
   1       18496624       4               0.0000%  36993265       4               0.0000%
   2       12             3              25.0000%  40             3               7.5000%
   3       12             3              25.0000%  40             3               7.5000%
   sum     35661590       14              0.0000%  71323246       14              0.0000%

   Note the behavior for each core is different, which is exactly what we need.

*** Problems
The are significant flaws in the proof of concept

1. Top of stack calculation
   This is currently done poorly in assembly (it must be done in assembly
   because the stack point cannot be initialized without knowing were the top of
   it should be). Figuring out a way to do this dynamically based on the
   available memory and the number of cores is extremely desirable.

2. global pointer allocation
   The global pointers are allocated 1024 bytes, this could be under or over
   what we may need. These should be dynamically sized based on the need of
   information to be stored there. However, this will be a challenge similar to
   the top of stack calculation to get the pointers stored in each core on init.

3. Synchronization
   There is no synchronization mechanism yet.



** Spin Locking
The protocol has been verified, but using spin locks will not work for tracking
instruction cache misses because the spin-locks will always hit the cache.

The solution is to use WFI (wait for interrupt) instead of spin-locking, and
then signal between cores. WFI is a simple instruction, cross hart interrupts
are the more challenging part. The solution is the MSWI mapping. A memory
mapping of one IPI register (MSIP) per hart.

| OFFSET | NAME     | PURPOSE                 |
|--------+----------+-------------------------|
| 0x0000 | MSIP0    | IPI Register for hart 0 |
| 0x0004 | MSIP1    | IPI Register for hart 1 |
|    ... | ...      | ...                     |
| 0x3FFC | RESERVED |                         |

Each register is 32 bits, with the least significant bit indicating whether or
not there's an interrupt to handle.

From the .dts file this appears to start at 0x2000000

* Cache Mapping
<<cache_mapping>>
Using the SIFIVE Red THING-V, with the FE302 G002 processor as a guide. The
following analysis of cache mapping has been tested:

With 16KB of 2-way cache, that's 14 bits to identify every
byte. That leaves 8KB per cache way, or 13 bits. Each block is 64
bytes, that's 6 bits. The index is the 7 bits.

Each 32 bit address and their masks are:

| TAG                     | INDEX       | OFFSET      |
| 19 bits                 | 7 bits      | 6 bits      |
| 1111 1111 1111 1111 111 | 1 1111 11   | 11 1111     |
| 0xFFFF 7000             | 0x0000 1FC0 | 0x0000 003F |

| Address     | TAG      | INDEX | OFFSET | Note             |
| 0x8000 0000 | 0x4000 0 |  0x00 |   0x00 | INDEX 0 START    |
| 0x8000 003F | 0x4000 0 |  0x00 |   0x3F | INDEX 0 END      |
| 0x8000 0040 | 0x4000 0 |  0x01 |   0x00 | INDEX 1 START    |
| 0x8000 007F | 0x4000 0 |  0x01 |   0x3F | INDEX 1 END      |
| 0x8000 0080 | 0x4000 0 |  0x02 |   0x00 | INDEX 2 START    |
| 0x8000 00BF | 0x4000 0 |  0x02 |   0x3F | INDEX 2 END      |
| 0x8000 00C0 | 0x4000 0 |  0x03 |   0x00 | INDEX 3 START    |
| 0x8000 00FF | 0x4000 0 |  0x03 |   0x3F | INDEX 3 END      |
| ...         | ...      |   ... |    ... | ...              |
| 0x8000 1FC0 | 0x4000 0 |  0x7F |   0x00 | INDEX 8191 START |
| 0x8000 1FFF | 0x4000 0 |  0x7F |   0x3F | INDEX 8191 END   |
| 0x8000 2000 | 0x4000 1 |  0x00 |   0x00 | INDEX 0 START    |
| 0x8000 203F | 0x4000 1 |  0x00 |   0x00 | INDEX 0 END      |


The following addresses should collide
0x8000 0000   0x8000 2000  0x8000 4000
0x8000 1FFF   0x8000 3FFF  0x8000 5FFF

** QEMU Cache Plugin Parameters
To match the cache configuration of the Thing-V, the following command line
options model the behavior

--plugin libcache.so,\
dcachesize=16384,dassoc=2,dblksize=32,\
icachesize=16384,iassoc=2,iblksize=32

* Block Reload Time
According to the documentation for the processor, it runs either at 1.6MHz or
2.73Mhz. Accessing values out of flash memory involves sending the information
over the QSPI interface; this is a serial bus that transfers data at a maximum
rate of 1 bit per serial clock tick (sck).

QSPI has a maximum clock speed of 100MHz, which would be too fast for the
FE310. The serial clock is also _slower_ than the input clock, and is
configured by a divisor. The input clock (tlclk) is set to the same speed as the
core clock (coreclk). Putting these together coreclk = tlclk = 2.73 Mhz (or
1.6MHz).

According to section 19.4 of the manual, where div's default value is 3.

             tlclk              2.73
  sclk = -------------  -->  ---------- =~ .34 Mhz
         2 ( div + 1 )           8

Then

  2.73
  ---- =~ 8.0 coreclk cycles per sclk cycles
   .34

The block size of flash, memory, and cache is 32 bytes, or 256 bits. Any cache
miss will take 256 sclk cycles or

  256 * 8 = 2048 coreclk cycles per cache miss.

This value closely matches the emepirically observed time of 2127 coreclk
cylces.

[[https://cdn.sparkfun.com/assets/7/f/0/2/7/fe310-g002-manual-v19p05.pdf]]
[[https://cdn.sparkfun.com/assets/5/b/e/6/2/fe310-g002-ds.pdf]]
[[https://learn.sparkfun.com/tutorials/serial-peripheral-interface-spi/all]]

* MRTC Benchmarks from BUNDLEP
These need to be ported to get the base and gamma values.

<<mrtc_mapping>>
| Group | Bencmarks  |    | Cores | Ported |
|     1 | bs         |  1 | 1,2   | Y      |
|       | bsort100   |  2 | 3,4   | Y      |
|       | crc        |  3 | 5,6   | Y      |
|     2 | expint     |  4 | 1,2   | Y      |
|       | fft        |  5 | 3,4   | Y      |
|       | insertsort |  6 | 5,6   | Y      |
|     3 | jfdctint   |  7 | 1,2   | Y      |
|       | lcdnum     |  8 | 3,4   | Y      |
|       | matmult    |  9 | 5,6   | Y      |
|     4 | minver     | 10 | 1,2   | Y      |
|       | ns         | 11 | 3,4   | Y      |
|       | nsichneu   | 12 | 5,6   | Y      |
|     5 | qurt       | 13 | 1,2   | Y      |
|       | select     | 14 | 3,4   | Y      |
|       | simple     | 15 | 5,6   | Y      |
|     6 | sqrt       | 16 | 1,2   | Y      |
|       | statemate  | 17 | 3,4   | Y      |
|       | ud         | 18 | 5,6   | Y      |

To determine their base and incremental values, each benchmark will be executed
on two cores in a fork-join task with a single parallel section. The first core
listed in <<mrtc_mapping>> will execute the benchmark once, the second core will
execute the benchmark twice. The difference in the number of cache misses will
be used to calculate the base and incremental costs.

For example, these are the schedules associated with the group 1 benchmarks.

| Core   | fj:0 | p:0      | fj: 1 | p:1               | fj:2 | costs |
| core 1 | -    | bs       | -     | -                 | -    | B     |
| core 2 | -    | -        | -     | bs bs             | -    | B + G |
| core 3 | -    | bsort100 | -     | -                 | -    | B     |
| core 4 | -    | -        | -     | bsort100 bsort100 |      | B + G |
| core 5 | -    | crc      | -     | -                 | -    | B     |
| core 6 | -    | -        | -     | crc crc           | -    | B + G |
B + G - B = G

* Synthetic Object Creation

b = base number of cycles
i = incremental cost (percentage of b)
s = number of = 2 * associativity
     splits
Sb = block size
Si = instruction size

                      Sb
ib = instructions  = ---- = ib
      per block       si


Cb =   cycles  = brt + cpi * ib
     per block

                     b
Bb = blocks per   = ----
     base cycles     Cb


Be =  blocks = Bb * i
     evicted

                    Be
Bs = blocks per =  ----
       split        s

Bp =   blocks   = Bb - Be
     persisted

     | Split 1 | Cache Line |   | Split 2  | Cache Line |   | Split ... |
     |---------+------------+---+----------+------------+---+-----------|
     | 0x0000  | 0          |   | 0x2000   | 0          |   |           |
     | 0x0040  | 1          |   | 0x2040   | 1          |   |           |
     | ...     | ...        |   | ...      | ...        |   | ...       |
     |         | Bs         |   |          | Bs         |   |           |
     |---------+------------+---+----------+------------+---+-----------|
     | 0x????  | Bs + 1     |   | Not used |            |   | Not Used  |
     | ...     |            |   |          |            |   |           |
     | 0x????  | Bs + Bp    |   |          |            |   |           |

** An example

Using the FE302 as an example

Base                  : 350,000 cycles
Incr                  : .75
cpi                   : 1
brt                   : 2048
Splits s : 2 * 2-way  : 4 splits

Block Size Sb         : 32 bytes
Instruction Size Si   : 32 bits
Instructions/Block ib : Sb / Si = 8

Cycles Per Block   Cb : brt + (cpi * ib) = 2048 + 1 * 8 = 2056
Blocks of Base     Bb : 350,000 / 2056 = 170
Blocks Evicited    Be : Bb * incr = 170 * .75 = 128
Blocks/Split       Bs : Be / s = 128 / 4 = 32
Blocks Persisted   Bp : Bb - Be = 170 - 128 = 42

|           | Split 1 | Split 2 | Split 3 | Split 4 |
| Evicted   |  0x0000 |  0x2000 |  0x4000 |  0x6000 |
| Blocks    |     ... |         |         |         |
|           |  0x0FFF |  0x2FFF |  0x4FFF |  0x6FFF |
|-----------+---------+---------+---------+---------|
| Persisted |  0x1000 |         |         |         |
| Blocks    |     ... |         |         |         |
|           |  0x1540 |         |         |         |

** Object Distributions

<<rep_objs>>
| Bases     | Incrs    | Names                |
| 50,000    | .3,.5,.8 | obj5a3,obj5a5,obj5a8 | Not used.
| 500,000   | .3,.5,.8 | obj5x3,obj5x5,obj5x8 |
| 1,000,000 | .5,.8    | obj1x5,obj1x8        |
| 2,000,000 | .8       | obj2x8               |

Justification for incremental values: the benchmarks show (experimentally)
incremental costs ranging from ~.01% to ~83%. Experiments recorded an incremental cost
of less than 1% for 9 of the 18 benchmarks. There were 4 benchmarks with
incremental costs above 25%. The incremental cost values for the representative
objects are conservative with respect to the experimentally observed values.

Justification for the bases: The benchmarks base costs range from approximately
49,000 to 1,800,000 cycles *including the synchronization costs*. An
empirical estimate of the synchronization costs is 25,000 cycles. To limit the
impact of synchronization the base cost minimum is set to 500,000 cycles. Due to
the conservative construction of the representative objects and the 8192 byte
cache way size, a 1,000,000 cycle task cannot be constructed with an incremental
cost below 47.3%. The MRTC benchmark nsichneu has a base cost of 1,800,000
cycles and an incremental cost of 70%, a representative object cannot be
constructed to match; it must have an incremental cost greater than 70%. A
500,000 cycle base cost was chosen as an approximate midpoint between the two
boundary base costs.

*** Example Citation
Uses the MRTC benchmarks ranging from ~1,000 to ~9,000,000 cycles.

@ARTICLE{9991138,
  author={Segarra, Juan and Martí-Campoy, Antonio},
  journal={IEEE Access},
  title={Improving the Configuration of the Predictable ACDC Data Cache for Real-Time Systems},
  year={2022},
  volume={10},
  number={},
  pages={132708-132724},
  doi={10.1109/ACCESS.2022.3230068}}

*** Candidates
task.126.json
task.947.json -- really good example
task.218.json -- perfect
task.273.json -- even more perfect
task.907.json -- okay
~
** Evaluation Directories
Benchmark base and incremental cost analysis:
fj-g1 fj-g2 fj-g2

** Cherry Picked Benchmark Straw Man
fj-task-284-*
2-Gram cannot schedule this task, 3-Parm and 3-Parm-HD can schedule it on two
cores.

** Representative Fork-Join Tasks

Schedulability
|             | fj-task-94-* | fj-task-273-* | fj-task-346-* |
| 2-gram      | unsched      | unsched       | sched         |
| 3parm       | unsched      | sched         | sched         |
| 3parmhd     | sched        | sched         | sched         |
| exactnocolo | sched        | -             | sched         |
| exactcolo   | sched        | -             | sched         |
| Note        | N1           | N2            | N3            |

| task          | 2-gram  | 3-parm  | 3-parm-hd | ExactNC | ExactC | Note |
| fj-task-94-*  | unsched | unsched | sched     |         |        | N1     |
| fj-task-273-* | unsched | sched   | sched     |         |        | N2     |
| fj-task-346-* | sched   | sched   | sched     |         |        | N3     |

N1: 3-parm and 2-gram are unschedulable due to the pathological parallel
sections. (in their best cases) Both place the 1,000,000 cycle objects on a core
with another object. Where 3-parm-hd isolates the 1,000,000 cycle objects. Exact
Colo and Exact No Colo produce the same schedule because the 1,000,000 cycle
object fills the available makespan of the segment.

N2: 2-gram is unschedulable due to it's inability to produce a makespan
less than 4,000,000 cycles. Only through co-location is such a schedule
possible. 3-parm and 3-parm-hd find the same schedule with a makespan of
2,950,000 cycles. For this single section with 21 threads, ExactNoColo did not
terminate within 72 hours.


N3: 2-gram schedules the task with 4 cores and a WCET of 3,000,000
cycles. 3-parm schedues the task with 4 cores and a WCET of 2,650,000
cycles. 3-parm-hd schedules the task with 3 cores and a WCET of 2,800,000
cycles. 3-parm-hd co-locates more objects, extending the makespan in favor of
fewer totla cores. The exact solutions find the same schedule as the 2,000,000
cycle task must be isolated to make the deadline

Observed Cycles of Core with Maximum Cycles

|             | fj-task-94-* | fj-task-273-* | fj-task-346-* |
| 2-gram      | 2,699,532    | 4,149,155     | 2,904,331     |
| 3parm       | 2,691,303    | 4,110,267     | 2,599,211     |
| 3parmhd     | 1,657,187    | 4,108,208     | 2,599,211     |
| exactnocolo | 2,177,291    | -             | 2,812,074     |
| exactcolo   | 2,177,291    | -             | 2,812,059     |
| Note        | N4           |               |               |

N4: The 3parm and 3parm hd schedules saw a decrease in cycles because the
first core, responsible for executing the fork-join nodes, shared cache values
between the fork-join nodes and the parallel section. Where the exact schedules
did not share cached values.

Observed Total Cycles

|             | fj-task-94-* | fj-task-273-* | fj-task-346-* |
| 2-gram      | 8,266,155    | 19,002,969    | 7,436,861     |
| 3parm       | 7,754,165    | 16,840,269    | 7,119,421     |
| 3parmhd     | 6,674,966    | 16,838,234    | 6,720,061     |
| exactnocolo | 8,355,803    | -             | 7,033,381     |
| exactcolo   | 8,355,745    | -             | 7,033,381     |
| Note        |              |               |               |


Cache (hits:misses)
|             | fj-task-94-* | fj-task-273-* | fj-task-346-* |
| 2-gram      | 29175 : 4020 | 66111 : 9242  | 27677 : 3616  |
| 3parm       | 29435 : 3770 | 67179 : 8186  | 27832 : 3461  |
| 3parmhd     | 25961 : 3245 | 67179 : 8185  | 28027 : 3266  |
| exactnocolo | 29213 : 4040 | -             | 27778 : 3419  |
| exactcolo   | 29189 : 4040 | -             | 27778 : 3419  |
| Note        |              |               |               |



* Subdirectories
** ref
Contains reference materials (PDFs)
** exit
Demonstrates how to exit the QEMU emulator.
** 00: two-threads
The basics running distinct control paths per hart (core)
** 01: lock-step
Spin-lock version of the lock-step protocol for managing cores as fork-join
task.
** 02: ipi
Inter Process Interrupt example
** 03: fj-poc
Half spin-lock version of the lock-step protocol, where execution cores wait on
interrupts and the controller spins.
** 04: fj-poc-v2
Interrupt based version of the lock-step protocol
** 05: fj-poc-v2-verify
Verifies the inter-thread cache benefit in the emulator.
** 06: fj-16
Skeleton of the fork-join task with a maximum of 16 objects (this matches the
synthetic tests).
** 07: fj-g1, fj-g2, fj-g3, fj-g4, fj-g5, fj-g6
Ports of the MRTC benchmarks according to [[mrtc_mapping]]
** 08: fj-blank
Debugging directory to fix the protocol, fixed in the fj-g1 -> fj-g6 directories
** 09: bin
Python scripts for collecting data
** 10: data
Directory to hold results
** 11: fj-b1 fj-b2
Basic implementation of [[rep_objs]]
** 12: fj-task-94-*
Representative task formed from task.94.json subject to scheduling by
exact no colo, exact colo, 2 graham, 3-parm, 3-parm-hd
** 13: fj-task-346-*
Representative task formed from task.346.json subject to scheduling by
exact no colo, exact colo, 2 graham, 3-parm, 3-parm-hd
** 14: fj-task-273-*
Representative task formed from task.346.json subject to scheduling by
2 graham, 3-parm, 3-parm-hd
** 15: fj-task-284-*
Example of bias towards our approach with the MRTC benchmarks, formed from
task.284.json subject to scheduling by
2 graham, 3-parm, 3-parm-hd
** 16: rep-task-metrics
Directory for calculating the metrics used in the synthetic evaluation over the
representative and MRTC based tasks. Must be in the same structure as the
evaluation expects.
** 17: fj-task-752
One MRTC conversion
*** 18: 2gram
*** 19: 3parm
*** 20: 3parmd
** 21: fj-task-916
MRTC Task with maximum F value
-- invalid
** 22: fj-task-732
MRTC task with minimum F value
-- invalid
** 23: fj-task-789
MRTC task with mean F value
-- invalid
** 24: fj-task-791
MRTC task with maximum F value .953
 - [X] 2gram
 - [X] 3parm
 - [X] 3parmhd
** 25: fj-task-956
Find all object02's and replace them with object01's,
object02 - bsort100
object01 - bs
MRTC task with minimum F value .182
 - [X] 2gram
 - [X] 3parm
 - [X] 3parmhd
** 26: fj-task-484
MRTC task with mean F value .554
 - [X] 2gram
 - [X] 3parm
 - [X] 3parmhd


* References
1. QEMU
   1. [[https://risc-v-getting-started-guide.readthedocs.io/en/latest/linux-qemu.html][Running 64- and 32-bit RISC-V Linux on QEMU]]
      Includes tuples for QEMU and Linux
      Interestingly there's no the kernel build
   2. [[https://wiki.qemu.org/Features/SoftMMU]]
      Description of SoftMMU
   3. [[https://wiki.qemu.org/Documentation/TCG]]
      TCG Introduction
   4. [[https://www.qemu.org/docs/master/system/target-riscv.html]]
      QEMU RISC-V system targets
   5. [[https://www.qemu.org/docs/master/system/riscv/virt.html]]
      QEMU RISC-V virt system target
   6. [[https://lists.gnu.org/archive/html/qemu-devel/2019-07/msg02049.html]]
      Including a bios became the default after the twilco bare metal tutorial
      was finished.
   7. [[http://ibiblio.org/gferg/ldp/GCC-Inline-Assembly-HOWTO.html]]
      Debugging with GDB, shows how to attach to multiple threads.
2. Shakti
   Shakti appears to be a RISC-V processor that can be programmed onto an FPGA
   1. [[https://www.qemu.org/docs/master/system/riscv/shakti-c.html]]
      QEMU shakti support
   2. [[https://gitlab.com/shaktiproject/cores/shakti-soc/-/tree/master]]
      gitlab for shakti verilog compilation
   3. [[https://digilent.com/shop/arty-a7-100t-artix-7-fpga-development-board/]]
      Board that supports the shakti core
   4. [[https://c-class.readthedocs.io/en/latest/quickstart.html#run-smoke-tests]]
      shakti tutorial
3. [[https://github.com/riscv-collab/riscv-gnu-toolchain]]
   RISC-V gnu toolchain, not sure who maintains this with respect to the RISC-V
   organization.
4. [[https://wiki.gentoo.org/wiki/Embedded_Handbook/Tuples]]
   Specifying tuples for crossdev
5. Bare Metal
   1. <<twilco>> [[https://github.com/twilco/riscv-from-scratch]]
      This is a bare-metal tutorial for getting a RISC-V chip running *and* it uses
      the QEMU virt target which has 8 cores.
      This is very promising.
   2. [[https://wiki.osdev.org/RISC-V_Meaty_Skeleton_with_QEMU_virt_board]]
      Another bare-metal tutorial which uses the QEMU virt board, based on [[twilco]]
   3. [[https://theintobooks.wordpress.com/2019/12/28/hello-world-on-risc-v-with-qemu/]]
      Alternative bare metal tutorial
   4. [[https://mth.st/blog/riscv-qemu/]]
      A year after the twilco tutorial, covers some of the same material but
      disables the OpenSBI firmware.
      This should be combined with the twilco tutorial.
6. [[https://sourceware.org/binutils/docs/as/CFI-directives.html]]
   GNU AS CFI directives
   Unsure if these are necessary or can be removed.
7. [[https://linux-tips.com/t/how-can-i-disable-cfi-directives-on-gas-assembler-output/66][Disable CFI Directives]]
   Compiler option -fno-asynchronous-unwind-tables may disable CFI directives.
8. TCG plugins
   1. [[https://qemu.readthedocs.io/en/latest/devel/tcg-plugins.html]]
      QEMU TCG Plugins
      libcache may work for us here
   2. [[https://www.qemu.org/2021/08/19/tcg-cache-modelling-plugin/]]
      Instructions for testing the cache plugin
   3. [[https://patchew.org/QEMU/20210623125458.450462-1-ma.mandourr@gmail.com/]]
      Blog post regarding the cache plugin
   4. [[https://gist.github.com/i3abghany/95dbbd48b3a4d8ffe4350f6290ea6d37#file-gsoc-qemu-cache-modelling-tcg-plugin-md]]
      Git page for the Google Summer of Code libcache implementation
   5. [[https://guillon.github.io/qemu-plugins/]]
      Tutorial for authoring TCG plugins, may be helpful debugging the cache
      plugin.
9. Assembly
   1. [[https://github.com/riscv-non-isa/riscv-asm-manual/blob/master/riscv-asm.md]]
      RISC-V Assembly tutorial
   2. [[https://shakti.org.in/docs/risc-v-asm-manual.pdf]]
      RISC-V Assembly manual
   3. https://web.eecs.utk.edu/~smarz1/courses/ece356/notes/assembly/
      RISC-V assembly tutorial (a decent one)
   4. [[https://stackoverflow.com/questions/1902901/show-current-assembly-instruction-in-gdb]]
      How to show assembly
   5. [[https://stackoverflow.com/questions/29527623/in-assembly-code-how-cfi-directive-works/33732119#33732119]]
      How the CFI directives work
   6. [[http://ibiblio.org/gferg/ldp/GCC-Inline-Assembly-HOWTO.html]]
      Inline Assembly How To
10. [[http://ibiblio.org/gferg/ldp/GCC-Inline-Assembly-HOWTO.html]]
    GNU C Reference Manual
11. [[https://users.informatik.haw-hamburg.de/~krabat/FH-Labor/gnupro/5_GNUPro_Utilities/c_Using_LD/ldLinker_scripts.html]]
    Linker Script Tutorial
12. [[https://riscv.org/technical/specifications/]]
    RISC-V Specifications and ISA
13. spike
    A RISC-V simulator, may be worth looking into.
14. [[https://ieeexplore.ieee.org/document/9781893]]
    Integration of Pycachesim with QEMU
    May be useful in the future.
15. [[https://github.com/riscv/riscv-aclint/blob/main/riscv-aclint.adoc]]
    MISP memory mapped registers for inter processor interrupts
16. [[https://github.com/qemu/qemu/blob/master/hw/riscv/virt.c]]
    QEMU CLINT (ALINT?) implementation
17. [[https://marz.utk.edu/my-courses/cosc562/riscv/]]
    Booting and RISC-V interrupts, this looks very good.
    *High value content*
18. [[https://github.com/rust-embedded/qemu-exit]]
    Implementation of QEMU exit in rust
    I'm struggling to figure this out.



# Local Variables:
# fill-column: 80
# eval: (auto-fill-mode)
# End:
